# -*- coding: utf-8 -*-
"""neurox.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l1dOGNzXPILyLjJOGPiNNgupF818aANA
"""

get_ipython().system('git clone https://github.com/fdalvi/NeuroX.git')

#!git clone https://github.com/fdalvi/NeuroX.git
neurox = '/home/chous/neurox'

get_ipython().system('pip install -U transformers')
get_ipython().system('pip install -U huggingface_hub')



get_ipython().system('imblearn')
get_ipython().system('h5py')
get_ipython().system('pip install torch')
import torch
import h5py
import imblearn



import transformers

from tqdm import tqdm

get_ipython().system('pip install transformers')


import neurox.data.extraction.transformers_extractor as transformers_extractor
tqdm(transformers_extractor.extract_representations('bert-base-uncased',
    '/home/chous/trainin (1).txt',
    '/home/chous/activations.json',
    aggregation="average" #last, first

))

import neurox.data.extraction.transformers_extractor as transformers_extractor
tqdm(transformers_extractor.extract_representations('bert-base-uncased',
    '/home/chous/testin (1).txt',
    '/home/chous/activations_test.json',
    aggregation="average" #last, first

))

import neurox.data.loader as data_loader
activations, num_layers = data_loader.load_activations('/home/chous/activations.json', 768)

activations[0].shape

import neurox.data.loader as data_loader
activations_test, num_layers_t = data_loader.load_activations('/home/chous/activations_test.json', 768)

import neurox.data.loader as data_loader

import neurox

tokens = data_loader.load_data('/home/chous/trainin (1).txt',
                               '/home/chous/trainl (1).txt',
                               activations,
                               512, # max_sent_l
                               sentence_classification = True
                              )

tokens_test = data_loader.load_data('/home/chous/testin (1).txt',
                               "/home/chous/testl (1).txt",
                               activations_test,
                               512, # max_sent_l
                               sentence_classification = True
                              )

import neurox.data.representations as repr

import neurox.interpretation.utils as utils
X, y, mapping = utils.create_tensors(tokens, activations, '0')
label2idx, idx2label, src2idx, idx2src = mapping

import neurox.interpretation.utils as utils
X_test, y_test, mapping_test = utils.create_tensors(tokens_test, activations_test, '0')
label2idx_t, idx2label_t, src2idx_t, idx2src_t = mapping_test

import random

import neurox.interpretation.linear_probe as linear_probe
probe = linear_probe.train_logistic_regression_probe(X, y, lambda_l1=0.001, lambda_l2=0.001, learning_rate = 5e-5, num_epochs = 20)

scores = linear_probe.evaluate_probe(probe, X_test, y_test, idx_to_class=idx2label_t)
scores

ordering, cutoffs = linear_probe.get_neuron_ordering(probe, label2idx)

import neurox.interpretation.ablation as ablation

X_selected_10 = ablation.filter_activations_keep_neurons(X, ordering[:int(len(ordering)*0.1)])
probe_selected_10 = linear_probe.train_logistic_regression_probe(X_selected_10, y, lambda_l1=0.001, lambda_l2=0.001)
X_selected_10_test = ablation.filter_activations_keep_neurons(X_test, ordering[:int(len(ordering)*0.1)])
scores10 = linear_probe.evaluate_probe(probe_selected_10, X_selected_10_test, y_test, idx_to_class=idx2label_t)

X_selected_25 = ablation.filter_activations_keep_neurons(X, ordering[:int(len(ordering)*0.25)])
probe_selected_25 = linear_probe.train_logistic_regression_probe(X_selected_25, y, lambda_l1=0.001, lambda_l2=0.001)
X_selected_25_test = ablation.filter_activations_keep_neurons(X_test, ordering[:int(len(ordering)*0.25)])
scores25 = linear_probe.evaluate_probe(probe_selected_25, X_selected_25_test, y_test, idx_to_class=idx2label_t)



with open("/home/chous/res_sn_1.txt", 'w') as f:
  f.write('full\t' + str(scores)+'\n')
  f.write('0.10\t' + str(scores10)+'\n')
  f.write('0.25\t' + str(scores25)+'\n')





